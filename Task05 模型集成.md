# Task05 模型集成
## 1.机器学习模型集成
在机器学习中，常见的模型集成方法有：bagging,boosting,stacking,blending。其中，bagging非常具有代表性的算法分别是Random Forest（随机森林），boosting有代表性的算法有GBDT（Gradient Boosting Decision Tree，梯度提升决策树）。

## 2.深度学习模型集成
除了以上提到的机器学习集成方法，深度学习还有自己独特的集成方式。
### 2.1 Dropout
Dropout可以作为训练深度神经网络的一种技巧。在每个训练批次中，通过随机让一部分的节点停止工作。同时在预测的过程中让所有的节点都其作用。
```
class SVHN_Model1(nn.Module):
    def __init__(self):
        super(SVHN_Model1, self).__init__()
       
        self.cnn = nn.Sequential(
            nn.Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2)),
            nn.ReLU(),
            nn.Dropout(0.25),
            nn.MaxPool2d(2),
            nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2)),
            nn.ReLU(), 
            nn.Dropout(0.25),
            nn.MaxPool2d(2),
        )
        
        self.fc0 = nn.Linear(32*3*7, 11)
        self.fc1 = nn.Linear(32*3*7, 11)
        self.fc2 = nn.Linear(32*3*7, 11)
        self.fc3 = nn.Linear(32*3*7, 11)
        self.fc4 = nn.Linear(32*3*7, 11)
        self.fc5 = nn.Linear(32*3*7, 11)
    
    def forward(self, img):        
        feat = self.cnn(img)
        # print(feat.shape)
        feat = feat.view(feat.shape[0], -1)
        c0 = self.fc0(feat)
        c1 = self.fc1(feat)
        c2 = self.fc2(feat)
        c3 = self.fc3(feat)
        c4 = self.fc4(feat)
        c5 = self.fc5(feat)
        return c0, c1, c2, c3, c4, c5
```

### 2.2 TTA(Test Time Augmentation)
测试集数据扩增（Test Time Augmentation，简称TTA）也是常用的集成学习技巧，数据扩增不仅可以在训练时候用，而且可以同样在预测时候进行数据扩增，对同一个样本预测三次，然后对三次结果进行平均。
```
def predict(test_loader, model, tta=10):
    model.eval()
    test_pred_tta = None
    # TTA 次数
    for _ in range(tta):
        test_pred = []
   
    with torch.no_grad():
        for i, (input, target) in enumerate(test_loader):
            c0, c1, c2, c3, c4, c5 = model(input)
            output = np.concatenate([c0.data.numpy(), c1.data.numpy(),
                  c2.data.numpy(), c3.data.numpy(),
                  c4.data.numpy(), c5.data.numpy()], axis=1)
            test_pred.append(output)
       
    test_pred = np.vstack(test_pred)
    if test_pred_tta is None:
           test_pred_tta = test_pred
    else:
           test_pred_tta += test_pred
   
    return test_pred_tta
```
结果如下：<br/>
```
2.3763835430145264
0.9352538585662842
0.999983012676239
0.9645788669586182
0.9716504216194153
0.9953840374946594
0.996080219745636
0.8467894196510315
Epoch: 0, Train loss: 0.9914828800360361 	 Val loss: 1.0048337008953094
0.0275
0.9705588817596436
0.9851158261299133
0.993828296661377
0.9400467276573181
0.7621369957923889
0.8686692714691162
0.8821144104003906
0.8782826066017151
Epoch: 1, Train loss: 0.8972929535706838 	 Val loss: 0.949831974029541
0.0628
0.7503582835197449
0.9172987937927246
0.9357467293739319
0.8022060990333557
0.7195796370506287
0.7757022380828857
0.7584273219108582
0.8172392845153809
Epoch: 2, Train loss: 0.8291386817296346 	 Val loss: 0.8718721008300782
0.0778
0.7601023316383362
0.8302395939826965
0.7090899348258972
0.7534101009368896
0.9546785354614258
0.7325005531311035
0.7675149440765381
0.8201037049293518
Epoch: 3, Train loss: 0.7925311826864878 	 Val loss: 0.8817327060699462
0.1005
0.7167831063270569
0.7165862917900085
0.71628737449646
0.7411693930625916
0.8306808471679688
0.7225744128227234
0.7886598110198975
0.6942647099494934
Epoch: 4, Train loss: 0.7767738598982493 	 Val loss: 0.9093247561454773
0.1056
0.6827886700630188
0.7781748175621033
0.6778028011322021
0.7281360626220703
0.7270308136940002
0.7623429298400879
0.6671389937400818
0.7171522974967957
Epoch: 5, Train loss: 0.7595128239790598 	 Val loss: 0.8802714529037475
0.1072
0.654401421546936
0.8537821769714355
0.7210530638694763
0.6923108696937561
0.8551526665687561
0.8055618405342102
0.6995171904563904
0.6579388380050659
Epoch: 6, Train loss: 0.7466045849323273 	 Val loss: 0.8833521609306335
0.1148
0.8492679595947266
0.7507700324058533
0.9256115555763245
0.7277706265449524
0.7818378806114197
0.7439377903938293
0.7403771877288818
0.7574564814567566
Epoch: 7, Train loss: 0.739677878777186 	 Val loss: 0.8404011311531067
0.1093
0.6721181869506836
0.6383067965507507
0.7041501998901367
0.6758901476860046
0.6327695250511169
0.6456831097602844
0.6794840693473816
0.7237215042114258
Epoch: 8, Train loss: 0.7296646551291148 	 Val loss: 0.8357288739681243
0.1186
0.6939330697059631
0.6794630885124207
0.8039834499359131
0.8513043522834778
0.7265929579734802
0.5502356886863708
0.7493770718574524
0.6555464863777161
Epoch: 9, Train loss: 0.7242615253925323 	 Val loss: 0.8510047652721405
0.1197
```
跟之前的未dropout差距很大，还需要继续尝试，调节参数。

## 3.总结
深度学习的集成算法之前完全没了解，至于上面的，随机去除掉一部分节点的做法，为何效果不好，还需要进一步了解和学习。<br/>
立个flag，6月份把整个文档补全，竞赛思路都尝试一遍。
